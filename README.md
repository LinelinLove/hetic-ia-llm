# hetic-ia-llm

> Mod√®le : llama3.2 de Ollama

## Lancer le projet avec Docker

PS : Sur Windows ouvrir Docker Desktop au pr√©alable

1. Cloner le projet : `git clone https://github.com/LinelinLove/hetic-ia-llm.git`

2. Aller dans le r√©pertoire du projet : `cd hetic-ia-llm`

3. Build et lancer le conteneur : `docker-compose up -d --build`

4. Attendre que le chargement soit termin√© jusqu'√† qu'il a ces messages suivants (l'installation du mod√®le llama3.2 peut prendre du temps) :

> Regarder les logs avec : `docker-compose logs -f` ou directement sur docker-desktop

```
writing manifest
success
ü§ñ Assistant Ollama - Tapez 'exit' pour quitter
-----------------------------------
```

## 1. Sans RAG

> IA g√©n√©rative

Le projet est pr√™t √† √™tre lanc√© avec la commande suivante :

`docker-compose exec python-ollama python app.py`

Vous pouvez √† pr√©sent donnere des prompts √† l'IA !
(L'IA met du temps √† r√©pondre 20sec √† 2min)

### Changer la temp√©rature de l'IA

Vous pouvez √©galement changer la temp√©rature de l'IA (par d√©faut `TEMPERATURE=0.5`) :

> Privil√©giez des valeurs entre 0.0 (+ rigide) et 1.0 (+ cr√©atif) pour √©viter des textes impr√©visibles

`docker-compose exec -e TEMPERATURE=0.5 python-ollama python app.py`

## 2. Avec RAG

> IA g√©n√©rative avec extraction de texte (avec Google Cloud et Google Drive)

1. Sur `https://console.cloud.google.com/`, cr√©er un projet puis : [Identifiants](https://console.cloud.google.com/apis/credentials)

2. `Cr√©er des identifiants` puis copier la cl√© API

3. Puis dans [API et services](https://console.cloud.google.com/apis/dashboard), cliquer sur `Activer les API et les services`

4. Dans [Biblioth√®que](https://console.cloud.google.com/apis/library), chercher et cliquer sur Google Drive API (dans Google workspace) et cliquer sur `Activer`

5. Renommer `.env.example` par `.env`et ajouter votre cl√© API

6. Dans votre Google Drive, importer un fichier PDF (plus le fichier est court, plus le traitement sera rapide) et g√©n√©rer un lien en public

7. Puis r√©cup√©rer le `file_id` de votre fichier a partir du lien g√©n√©r√© comme ci-dessous:

```
https://drive.google.com/file/d/file_id/view
```

et le coller dans le `.env` dans l'emplacement pr√©vu

7. Lancer la commande : `docker-compose exec python-ollama python rag.py`

Votre fichier est install√© depuis votre Google Drive

Vous pouvez √† pr√©sent poser des questions sur le fichier PDF que vous avez fourni ! (L'IA peut mettre 2min √† r√©pondre)

> Si les valeurs `FILE_ID`et `CLOUD_API_KEY`du .env n'ont pas √©t√© d√©fini, l'IA analysera le fichier qui est par d√©faut dans le dossier pdf

### 2√®me m√©thode pour analyser un fichier

Vous pouvez pr√©ciser directement le `FILE_ID` et `CLOUD_API_KEY` dans la commande :

```
docker-compose exec -e FILE_ID=placeholder_for_file_id -e CLOUD_API_KEY=placeholder_for_cloud_api_key python-ollama python rag.py
```

> Si il y a d√©j√† `FILE_ID` dans le fichier `.env`, le `FILE_ID` prise en compte sera celle de la commande, de m√™me pour `CLOUD_API_KEY`

> Vous pouvez √©galement mettre `CLOUD_API_KEY` dans le `.env` et juste modifier `FILE_ID` dans la commande

```
docker-compose exec -e FILE_ID=placeholder_for_file_id python-ollama python rag.py
```

Vous pouvez √©galement changer la temp√©rature en rajoute `-e TEMPERATURE=value`

```
docker-compose exec -e FILE_ID=placeholder_for_file_id -e CLOUD_API_KEY=placeholder_for_cloud_api_key -e TEMPERATURE=0.5 python-ollama python rag.py
```
